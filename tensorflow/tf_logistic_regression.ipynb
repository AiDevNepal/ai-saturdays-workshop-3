{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mnist Dataset Classification using Logistic Regression\n",
    "\n",
    "Let's only use two digits to reduce the problem to binary classification because we are using Logistic Regression that is used for linearly separable problem.\n",
    "\n",
    "### Input X\n",
    "It is an array/matrix of **N** number of rows and **M** columns i.e. NXM matrix.\n",
    "\n",
    "Each row represent a single data instance that is a single digit. Each row is a vector of length **M**. So, there are **M** feature for a single data instance.\n",
    "\n",
    "[  \n",
    "    [x00, x01, x02, ..., x0m],  \n",
    "    [x10, x11, x12, ..., x1m],  \n",
    "    [x20, x21, x22, ..., x2m],  \n",
    "    ...,  \n",
    "    ...  \n",
    "    [xn0, xn1, xn2, ..., xnm]  \n",
    "]  \n",
    "\n",
    "### Target Y\n",
    "Each image represent a certain digit from 0 to 9. So, we have labelled data for performing **Supervised Classification**.\n",
    "\n",
    "Generally, we encode each label into **one-hot-binary-vector** of length **k**. **One-hot-label** is a vector of length equal to the number of classes and each element represents a single class. The vector contains value **1** in the element that represents the respective class it belongs to. Like:  \n",
    "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0] --> digit '0'  \n",
    "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0] --> digit '1'  \n",
    "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0] --> digit '2'  \n",
    "...  \n",
    "...  \n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1] --> digit '9'\n",
    "\n",
    "### In our following example\n",
    "- We will only be classifying only two digits: 0 and 1\n",
    "- Each image is of 8x8 size\n",
    "- After vectorizing, the resulting vector is of length 64. That is there are 64 pixels in the  image.\n",
    "- Each pixel's value is in the range [0, 255]. Hence, each image is a grayscale image.\n",
    "- Each image is labelled either 0 or 1 that represent a digit **zero** or **one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y :\n",
      "The shape of Y is :  (360,)\n",
      "1st 10 elements of Y is :  [0 1 0 1 0 1 0 0 1 1]\n",
      "X: \n",
      "The shape of X is :  (360, 64)\n",
      "Each X is of shape :  (64,)\n",
      "An example of X is : \n",
      " [ 0.  0.  1.  9. 15. 11.  0.  0.  0.  0. 11. 16.  8. 14.  6.  0.  0.  2.\n",
      " 16. 10.  0.  9.  9.  0.  0.  1. 16.  4.  0.  8.  8.  0.  0.  4. 16.  4.\n",
      "  0.  8.  8.  0.  0.  1. 16.  5.  1. 11.  3.  0.  0.  0. 12. 12. 10. 10.\n",
      "  0.  0.  0.  0.  1. 10. 13.  3.  0.  0.]\n",
      "Its respective label is :  0\n"
     ]
    }
   ],
   "source": [
    "# load only two-digits dataset by passing the argument '2'\n",
    "digits = load_digits(2)\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "\n",
    "print(\"Y :\")\n",
    "print(\"The shape of Y is : \", Y.shape)\n",
    "print(\"1st 10 elements of Y is : \", Y[:10])\n",
    "\n",
    "print(\"X: \")\n",
    "print(\"The shape of X is : \", X.shape)\n",
    "print(\"Each X is of shape : \", X[0].shape)\n",
    "print(\"An example of X is : \\n\", X[2])\n",
    "print(\"Its respective label is : \", Y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff673c7b470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff673d24ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[3].reshape([8,8]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff673c93668>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff673c46c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0].reshape([8,8]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Use Hold Out Method\n",
    "Instead of using all the training data, we use some major portion of it for training and remaining for testing the accuracy of the model. This is called **holdout** method.\n",
    "\n",
    "So, we create **Train Set** and **Test Set**.\n",
    "\n",
    "#### Train Set\n",
    "Used for actually building the machine leanring model. It is generally 70% of total data instance.\n",
    "\n",
    "#### Test Set\n",
    "It is used for evaluating the built model. It is generally 30% of total data instance. That is:  \n",
    "`test_set = whole_data - train_set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do it, alright!\n",
    "\n",
    "First, we have to use sigmoid to predict the probability of a label.\n",
    "$$\n",
    "P(y=1|X) = \\sigma(X \\cdot W + b)\n",
    "$$\n",
    "\n",
    "Then, we have to calculate the error using **cross-entropy loss**. Cross entropy cost function is used for probabilistic distribution.  \n",
    "  \n",
    "  \n",
    "$$\n",
    "\\text{loss} = -\\log\\left(P\\left(y_\\text{predicted} = 1\\right)\\right)\\cdot y_\\text{true} - \\log\\left(1 - P\\left(y_\\text{predicted} = 1\\right)\\right)\\cdot\\left(1 - y_\\text{true}\\right)\n",
    "$$\n",
    "\n",
    "$\\sigma(x)$ is available via `tf.nn.sigmoid` and matrix multiplication via `tf.matmul`\n",
    "\n",
    "### Note:\n",
    "Generally, we use **softmax activation** instead of **sigmoid** with the cross-entropy loss because softmax activation distributes the probability througout each output node.  \n",
    "\n",
    "But, since it is a binary classification, using sigmoid is same as softmax. For multi-class classification use sofmax with cross-entropy.\n",
    "\n",
    "In case you are wondering why using sigmoid is identical to softmax for binary (single-class) classification, refer to this:  \n",
    "https://stats.stackexchange.com/questions/207049/neural-network-for-binary-classification-use-1-or-2-output-neurons/207067#207067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weights:0' shape=(64, 1) dtype=float32_ref>\n",
      "<tf.Variable 'b:0' shape=() dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# create global variables for weights and bias which have to be updated iteratively\n",
    "weights = tf.Variable(dtype=tf.float32, initial_value=2*np.random.random((X.shape[1], 1))*0.001, name='weights')\n",
    "b = tf.Variable(dtype=tf.float32, initial_value=1, name='b')\n",
    "\n",
    "print(weights)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_x:0\", dtype=float32)\n",
      "Tensor(\"input_y:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# create a dummy value representing x and y data\n",
    "input_X = tf.placeholder(dtype=tf.float32, name='input_x')\n",
    "input_Y = tf.placeholder(dtype=tf.float32, name='input_y')\n",
    "\n",
    "print(input_X)\n",
    "print(input_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "In a machine learning problem, the main goal is to minimize the error/loss. There are various ways to minimize the loss.\n",
    "\n",
    "Generally, we try to find minima of the loss function. In a 2d space, we do it by finding the point where first derivative of that curve/function is zero.  \n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "y = (x-2)^2\n",
    "$$\n",
    "\n",
    "Here,\n",
    "$$\n",
    "dy/dx = 2 * (x-2)\n",
    "$$\n",
    "\n",
    "So, for finding minima\n",
    "$$\n",
    "dy/dx = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "2 * (x-2) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "x = 2\n",
    "$$\n",
    "\n",
    "The scenario is similar in higher dimension. \n",
    "\n",
    "### But\n",
    "The whole point of using machine learning is to find a **glorified curve** that really fits our data. So, we don't have enough power to ideally evaluate the curve to find that solution. That is why the straight-forward solution mentioned above is quite painful.\n",
    "\n",
    "The weights are randomly initialized because we don't know the ideal values. So, we employ a certain method/technique that allows us to start from a random step and slowly move towards  the minima.  \n",
    "\n",
    "**model = f(w1, w2, w3, ..., wn)**\n",
    "\n",
    "## Gradient Descent\n",
    "It is one of the widely used method for optimization of a machine learning model. \n",
    "\n",
    "The game is like a hill-climbing race. We have to reach the top of a hill fast.\n",
    "- we start from  a random point/position: `weight = w0`\n",
    "- we find the **most-steep** direction. This is done using a gradient since a gradient points to the direction of steepest ascent.\n",
    "- We take a small step in that direction. We reach a new position.\n",
    "- On this new position we repeat the process until we are no longer pointing anywhere.\n",
    "\n",
    "So, gradient descent is like this but imagine it to be inverted  hill and you have to find the bottom of that hill.\n",
    "So, instead of going in the direction of the  gradient, we move in the opposite direction to it.\n",
    "\n",
    "$$\n",
    "w new = w old - rate * gradient\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Squeeze:0\", dtype=float32)\n",
      "Tensor(\"Neg:0\", dtype=float32)\n",
      "name: \"GradientDescent\"\n",
      "op: \"NoOp\"\n",
      "input: \"^GradientDescent/update_weights/ApplyGradientDescent\"\n",
      "input: \"^GradientDescent/update_b/ApplyGradientDescent\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_y = tf.squeeze(tf.nn.sigmoid(tf.add( tf.matmul(input_X, weights) , b)))\n",
    "print(predicted_y)\n",
    "\n",
    "loss = -tf.reduce_mean(input_Y*tf.log(predicted_y) + (1-input_Y)*tf.log(1-predicted_y))\n",
    "print(loss)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run The Graph In A Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0: 2.2119\n",
      "train auc: 0.9950592885375493\n",
      "test auc: 0.999505928853755\n",
      "loss at iter 1: 1.5132\n",
      "train auc: 0.999505928853755\n",
      "test auc: 0.9990118577075099\n",
      "loss at iter 2: 0.3817\n",
      "train auc: 1.0\n",
      "test auc: 1.0\n",
      "loss at iter 3: 0.0110\n",
      "train auc: 1.0\n",
      "test auc: 1.0\n",
      "loss at iter 4: 0.0104\n",
      "train auc: 1.0\n",
      "test auc: 1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(5):\n",
    "        sess.run(optimizer, {input_X: X_train, input_Y: Y_train})\n",
    "        loss_i = sess.run(loss, {input_X: X_train, input_Y: Y_train})\n",
    "        print(\"loss at iter %i: %.4f\" % (i, loss_i))\n",
    "        print(\"train auc:\", roc_auc_score(Y_train, sess.run(predicted_y, {input_X:X_train})))\n",
    "        print(\"test auc:\", roc_auc_score(Y_test, sess.run(predicted_y, {input_X:X_test})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
